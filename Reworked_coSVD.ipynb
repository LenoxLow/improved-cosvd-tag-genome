{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import (absolute_import, division, print_function,\n",
    "                        unicode_literals)\n",
    "\n",
    "#cimport numpy as np # noqa\n",
    "import numpy as np\n",
    "\n",
    "from surprise import Reader, AlgoBase, PredictionImpossible\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.utils import get_rng\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, mean_absolute_error, precision_score, recall_score\n",
    "from math import sqrt\n",
    "\n",
    "import matrices_generation as mg\n",
    "\n",
    "#%reload_ext Cython\n",
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## co-SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "cimport cython\n",
    "cimport numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "from surprise import AlgoBase, PredictionImpossible\n",
    "from surprise.utils import get_rng\n",
    "from functools import reduce\n",
    "from cython.parallel import prange\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "\n",
    "class co_SVD(AlgoBase):\n",
    "    def __init__ (\n",
    "        self\n",
    "        , n_factors = 40\n",
    "        , n_epochs = 1\n",
    "        , biased = True\n",
    "        , init_mean = 0\n",
    "        , init_std_dev=.1\n",
    "\n",
    "        , lr_all=.005\n",
    "        , reg_all=.02\n",
    "        , lr_bu=None\n",
    "        , lr_bi=None\n",
    "        , lr_bt=None\n",
    "        , lr_pu=None\n",
    "        , lr_qi=None\n",
    "        , lr_rt=None\n",
    "\n",
    "        , reg_p=.001\n",
    "        , reg_r=.035\n",
    "        , reg_f=1.5\n",
    "\n",
    "        , random_state=None\n",
    "        , verbose=False\n",
    "        , p_ut=None\n",
    "        , f_it=None\n",
    "        , tags=None\n",
    "        , ratings=None\n",
    "        ):\n",
    "\n",
    "        self.n_factors = n_factors\n",
    "        self.n_epochs = n_epochs\n",
    "        self.biased = biased\n",
    "        self.init_mean = init_mean\n",
    "        self.init_std_dev = init_std_dev\n",
    "        self.lr_bu = lr_bu if lr_bu is not None else lr_all\n",
    "        self.lr_bi = lr_bi if lr_bi is not None else lr_all\n",
    "        self.lr_bt = lr_bt if lr_bt is not None else lr_all\n",
    "\n",
    "        self.lr_pu = lr_pu if lr_pu is not None else lr_all\n",
    "        self.lr_qi = lr_qi if lr_qi is not None else lr_all\n",
    "        self.lr_rt = lr_rt if lr_rt is not None else lr_all\n",
    "\n",
    "        self.reg_p = reg_p\n",
    "        self.reg_f = reg_f\n",
    "        self.reg_r = reg_r\n",
    "\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.p_ut = p_ut\n",
    "        self.f_it = f_it\n",
    "        self.tags = tags\n",
    "        self.ratings = ratings\n",
    "        AlgoBase.__init__(self)\n",
    "        \n",
    "   \n",
    "  \n",
    "    def fit(self,trainset):\n",
    "        AlgoBase.fit(self, trainset)       \n",
    "        self.sgd(trainset)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def sgd(self, trainset):\n",
    "        cdef np.ndarray[np.double_t] bu\n",
    "        cdef np.ndarray[np.double_t] bi\n",
    "        cdef np.ndarray[np.double_t] bt\n",
    "\n",
    "        cdef np.ndarray[np.double_t, ndim=2] pu\n",
    "        cdef np.ndarray[np.double_t, ndim=2] qi\n",
    "        cdef np.ndarray[np.double_t, ndim=2] rt\n",
    "\n",
    "        cdef int u, i, t, f, raw_u, raw_i, raw_t\n",
    "        cdef double r, p_put, p_fit, err_r, err_p, err_f, dot_r, dot_p, dot_f, puf, qif, rtf, global_mean_p, global_mean_f\n",
    "        cdef double global_mean_r = self.trainset.global_mean\n",
    "\n",
    "        cdef double lr_bu = self.lr_bu\n",
    "        cdef double lr_bi = self.lr_bi\n",
    "        cdef double lr_bt = self.lr_bt\n",
    "\n",
    "        cdef double lr_pu = self.lr_pu\n",
    "        cdef double lr_qi = self.lr_qi\n",
    "        cdef double lr_rt = self.lr_rt\n",
    "\n",
    "        cdef double reg_p = self.reg_p\n",
    "        cdef double reg_f = self.reg_f\n",
    "        cdef double reg_r = self.reg_r\n",
    "\n",
    "        p_ut = self.p_ut\n",
    "        f_it = self.f_it\n",
    "        tags = self.tags\n",
    "        ratings = self.ratings\n",
    "        \n",
    "        cdef int n_factors = self.n_factors\n",
    "        raw_user = np.zeros(trainset.n_users, int)\n",
    "        for i in trainset.all_users():\n",
    "            raw_user[i] = trainset.to_raw_uid(i)\n",
    "\n",
    "        raw_item = np.zeros(trainset.n_items, int)\n",
    "        for i in trainset.all_items():\n",
    "            raw_item[i] = trainset.to_raw_iid(i)\n",
    "        \n",
    "        final_raw = ratings[ratings.userId.isin(raw_user) & ratings.movieId.isin(raw_item)]\n",
    "        raw_data = final_raw\n",
    "\n",
    "        uni_tid = raw_data.tid.unique()\n",
    "        uni_tid = uni_tid[~np.isnan(uni_tid)]\n",
    "        u_t = pd.DataFrame({'tid': uni_tid,\n",
    "                            'tid_inner':range(len(uni_tid))})\n",
    "        \n",
    "        raw_data = pd.merge(raw_data, u_t, how ='left', on=['tid'])\n",
    "        \n",
    "        final_p = p_ut[p_ut.userId.isin(raw_user) & p_ut.tid.isin(raw_data.tid)]\n",
    "        final_f = f_it[f_it.movieId.isin(raw_item) & f_it.tid.isin(raw_data.tid)]\n",
    "        \n",
    "        final_p = pd.merge(final_p, u_t, how='left', on=['tid'])\n",
    "        final_f = pd.merge(final_f, u_t, how='left', on=['tid'])\n",
    "\n",
    "        p_ut = final_p.drop(['tid'], axis=1)\n",
    "        f_it = final_f.drop(['tid'], axis=1)\n",
    "\n",
    "        rng = get_rng(self.random_state)\n",
    "        \n",
    "        bu = np.zeros(trainset.n_users, np.double)\n",
    "        bi = np.zeros(trainset.n_items, np.double)\n",
    "\n",
    "        bt = np.zeros(len(p_ut.tid_inner.unique()), np.double)\n",
    "        \n",
    "        pu = rng.normal(self.init_mean, self.init_std_dev,\n",
    "                        (trainset.n_users, n_factors))\n",
    "        qi = rng.normal(self.init_mean, self.init_std_dev,\n",
    "                        (trainset.n_items, n_factors))\n",
    "        rt = rng.normal(self.init_mean, self.init_std_dev,\n",
    "                        (len(p_ut.tid_inner.unique()), n_factors))\n",
    "        \n",
    "        global_mean_p = np.mean(p_ut.val)\n",
    "        global_mean_f = np.mean(f_it.val)\n",
    "        \n",
    "        for current_epoch in range(self.n_epochs):\n",
    "            if self.verbose:\n",
    "                print(\"Processing epoch {}\".format(current_epoch), end='\\r')\n",
    "\n",
    "            rv_sum = np.zeros((trainset.n_users, n_factors))\n",
    "            pz_sum = np.zeros((trainset.n_users, n_factors))\n",
    "            ru_sum = np.zeros((trainset.n_items, n_factors))\n",
    "            fz_sum = np.zeros((trainset.n_items, n_factors))\n",
    "            pu_sum = np.zeros((len(p_ut.tid_inner.unique()), n_factors))\n",
    "            fv_sum = np.zeros((len(p_ut.tid_inner.unique()), n_factors))\n",
    "            \n",
    "            for rn in range(len(raw_data)):\n",
    "                raw_u = raw_data.loc[rn, 'userId']\n",
    "                raw_i = raw_data.loc[rn, 'movieId']\n",
    "                \n",
    "                u = trainset.to_inner_uid(raw_u)\n",
    "                i = trainset.to_inner_iid(raw_i)\n",
    "                \n",
    "                r = raw_data.loc[rn, 'rating']\n",
    "                if pd.isna(raw_data.loc[rn, 'tid_inner']) | np.isnan(raw_data.loc[rn, 'tid_inner']):\n",
    "                    t = -1\n",
    "                else:\n",
    "                    t = raw_data.loc[rn, 'tid_inner']\n",
    "                \n",
    "                t = int(t)\n",
    "                \n",
    "                if t != -1:\n",
    "                    p_put = p_ut.val[(p_ut.userId == raw_u) & (p_ut.tid_inner == t)].values[0]\n",
    "                    p_fit = f_it.val[(f_it.movieId == raw_i) & (f_it.tid_inner == t)].values[0]\n",
    "                    \n",
    "                if math.isnan(r):\n",
    "                    r = 0\n",
    "\n",
    "                dot_r = 0.0\n",
    "                dot_p = 0.0\n",
    "                dot_f = 0.0\n",
    "                \n",
    "                for f in prange(n_factors, nogil=True):\n",
    "                    dot_r += qi[i,f] * pu[u,f]\n",
    "\n",
    "                if t != -1:\n",
    "                    for f in prange(n_factors, nogil=True):\n",
    "                        dot_p += rt[t,f] * pu[u,f]\n",
    "                        dot_f += rt[t,f] * qi[i,f]\n",
    "\n",
    "                \n",
    "                err_r = r - (global_mean_r + bu[u] + bi[i] + dot_r)\n",
    "\n",
    "                if t != -1:\n",
    "                    err_p = p_put - (global_mean_p + bu[u] + bt[t] + dot_p)\n",
    "                    err_f = p_fit - (global_mean_f + bi[i] + bt[t] + dot_f)\n",
    "                else:\n",
    "                    err_p = 0.0\n",
    "                    err_f = 0.0\n",
    "\n",
    "                if self.biased:\n",
    "                    bu[u] -= lr_bu * (-1 * err_r - reg_p * err_p + reg_r * bu[u])\n",
    "                    bi[i] -= lr_bi * (-1 * err_r - reg_f * err_f + reg_r * bi[i])\n",
    "                    if t != -1:\n",
    "                        bt[t] -= lr_bt * (-1 * reg_p * err_p - reg_f * err_f + reg_r * bt[t])\n",
    "                \n",
    "                if r != 0:\n",
    "                    rv_sum[u] += err_r * qi[i]\n",
    "                    ru_sum[i] += err_r * pu[u]\n",
    "                \n",
    "                if t != -1:\n",
    "                    pz_sum[u] += err_p * rt[t]\n",
    "                    fz_sum[i] += err_f * rt[t]\n",
    "                    pu_sum[t] += err_p * pu[u]\n",
    "                    fv_sum[t] += err_f * qi[i]\n",
    "                    \n",
    "            pu -= lr_pu * (-1 * rv_sum - reg_p * pz_sum + reg_r * pu)\n",
    "            qi -= lr_pu * (-1 * ru_sum - reg_f * fz_sum + reg_r * qi)\n",
    "            rt -= lr_pu * (-1 * reg_p * pu_sum - reg_f * fv_sum + reg_r * rt)\n",
    "                    \n",
    "        self.bu = bu\n",
    "        self.bi = bi\n",
    "        self.pu = pu\n",
    "        self.qi = qi\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        known_user = self.trainset.knows_user(u)\n",
    "        known_item = self.trainset.knows_item(i)\n",
    "\n",
    "        if self.biased:\n",
    "            est = self.trainset.global_mean\n",
    "\n",
    "            if known_user:\n",
    "                est += self.bu[u]\n",
    "\n",
    "            if known_item:\n",
    "                est += self.bi[i]\n",
    "                \n",
    "            if known_user and known_item:\n",
    "                est += np.dot(self.qi[i], self.pu[u])\n",
    "        else:\n",
    "            if known_user and known_item:\n",
    "                est = np.dot(self.qi[i], self.pu[u])\n",
    "            else:\n",
    "                raise PredictionImpossible('User and item are unknown')\n",
    "        return est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = 'mlsmall'          # MovieLens dataset 2018\n",
    "#data_source = 'ml-latest-small' # MovieLens dataset 2016\n",
    "\n",
    "reader = Reader()\n",
    "path = os.path.join('Dataset',data_source)\n",
    "rate = pd.read_csv(path+'/ratings.csv', encoding='utf-8')\n",
    "raw_tags = pd.read_csv(path+'/tags.csv', encoding='utf-8')\n",
    "\n",
    "data = Dataset.load_from_df(rate[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "## The Tag Genome dataset obtain from MovieLens latest 27M dataset\n",
    "if data_source == 'mlsmall':\n",
    "    path = os.path.join('Dataset', 'ml-latest')\n",
    "elif data_source == 'ml-latest-small':\n",
    "    path = os.path.join('Dataset', 'ml-latest-2016')\n",
    "    \n",
    "genome_tag = pd.read_csv(path+'/genome-tags.csv', encoding='utf-8')\n",
    "genome_score = pd.read_csv(path+'/genome-scores.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining Tag Records: 2171\n"
     ]
    }
   ],
   "source": [
    "gb_tags = raw_tags.groupby(['tag'], as_index=False)['userId'].count()\n",
    "raw_tags = raw_tags[raw_tags.tag.isin(list(gb_tags.tag[gb_tags.userId >= 3]))].reset_index(drop=True)\n",
    "print(\"Remaining Tag Records: \" + str(len(raw_tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr  = 0.006 # Learning Rate of the co-SVD\n",
    "epoch = 20 # The number of iteration for model training\n",
    "n_eval = 10 # The number of iteration for model evaluation\n",
    "factors = 40 # the number of latent features, may change to 20, 30 or 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ut, f_it, tags, ratings = mg.generateTagsOrigin(rate, raw_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MovieLens Dataset 2018\n",
      "Reworked co-SVD - Result\n",
      "precision    0.890244\n",
      "recall       0.774372\n",
      "rmse         0.637633\n",
      "mae          0.489004\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "res = pd.DataFrame(columns = ['n_eval', 'precision' , 'recall', 'rmse', 'mae'])\n",
    "\n",
    "for j in range(n_eval):\n",
    "    trainset, testset = train_test_split(data, test_size=.3, random_state=j)\n",
    "    algo = co_SVD(verbose=False, n_epochs=epoch, lr_all=lr, n_factors=factors\n",
    "                  , p_ut=p_ut, f_it=f_it, tags=tags, ratings=ratings, random_state=j\n",
    "                 )\n",
    "\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    mae = accuracy.mae(predictions, verbose=False)\n",
    "    rmse = accuracy.rmse(predictions, verbose=False)\n",
    "\n",
    "    test_res = pd.DataFrame(predictions)\n",
    "\n",
    "    threshold = 3.5\n",
    "    test_res['actual_cat'] = list(map(lambda x: 1 if x >= threshold else 0, test_res['r_ui']))\n",
    "    test_res['pred_cat'] = list(map(lambda x: 1 if x >= threshold else 0, test_res['est']))\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(test_res.actual_cat, test_res.pred_cat).ravel()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    res = res.append(pd.Series([j, precision, recall, rmse, mae], index=res.columns), ignore_index = True)\n",
    "    print(\"Eval: \" + str(j+1), end=\"\\r\")\n",
    "    \n",
    "if data_source == \"mlsmall\":\n",
    "    print(\"MovieLens Dataset 2018\")\n",
    "elif data_source == \"ml-latest-small\":\n",
    "    print(\"MovieLens Dataset 2016\")\n",
    "\n",
    "print(\"Reworked co-SVD - Result\")\n",
    "print(res.drop(columns=['n_eval']).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
